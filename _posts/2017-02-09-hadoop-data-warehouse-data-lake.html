---
layout: post
title: Hadoop? Data Warehouse? Data Lake?
date: '2017-02-09T19:55:00.000-08:00'
author: Karteek
tags: 
modified_time: '2017-02-09T19:55:16.057-08:00'
blogger_id: tag:blogger.com,1999:blog-8004810643159823472.post-3159290113876471489
blogger_orig_url: http://www.akyadavilli.xyz/2017/02/hadoop-data-warehouse-data-lake.html
---

<div dir="ltr" style="text-align: left;" trbidi="on"><h4 style="text-align: left;">Houston, We have a problem</h4>The data warehouse mission has always been to identify and leverage all possible data assets to derive actionable insights out of it&nbsp;and expose them in the most efficient manner without having to physically centralize the data. Given that, this model has been very successful, there has been a tendency to use this model to solve every problem that comes our way.<br /><br />Some are easy to solve and some turn out to be more than what can be handled. Unstructured, Semi-structured and machine log data are a few of several things that turned out to be more than handful. The solution to this problem has been identified to be Hadoop.<br /><h4>What's Hadoop?</h4><div>Hadoop, simply put, is an open source framework designed for distributed storage and data processing. From a traditional DW practitioner's perspective, just as we had multiple layers in RDBMS like User Tables, System Tables and Database Engine to take care of Data Storage, Metadata Storage and Query processing respectively, Hadoop Ecosystem has multiple layers like HDFS, HCatalog and Hive (Say) to do the same. One particular contrast worth mentioning is that given that the latter is open source, there has been a provision to expose each layer of this independently of the Query processing Engine.</div><h4 style="text-align: left;">So How to build a DW inside Hadoop?</h4><div>Many people think that we can build a DW inside Hadoop. That may even be a possibility but consider these things before you do that:&nbsp;</div><div><ul style="text-align: left;"><li>Dashboard user don’t want to wait 10+ seconds for a MapReduce job to start up to execute a Hive query</li><li>Hadoop lacks a query optimizer and indexing and performs poorly for complex queries</li><li>Hadoop is not relational, as all the data is in files in HDFS, so there always is a conversion process to convert the data to a relational format</li><li>Hadoop is not a database management system. &nbsp;It does not have functionality such as update of data, referential integrity, statistics, ACID compliance, data security, and the plethora of tools and facilities needed to govern corporate data assets</li><li>Many tools/technologies/versions/vendors (fragmentation), no standards, difficult to make a corporate standard</li><li>Some reporting tools don’t work against Hadoop, as well as some reporting tools require data in OLAP</li></ul><div>There are advancements made to Hadoop ecosystem such as introduction of Spark, Apache Kylin, Apache Zeppelin etc., that may enable bringing Hadoop close to replace DW completely. But, until then, it's recommended to use Hadoop as a Data lake and build materialized relational structures for analytical needs that source data from the Data lake.</div></div><h4 style="text-align: left;">Now What's the Data Lake?</h4><div>About that, Data lake is a dumpster that holds a vast amount of raw data in its native format until it is needed. To understand this mindset, let us examine the fundamental approach of how we've been doing Data Warehousing all this while:</div><div><ul style="text-align: left;"><li>Identify business questions to answer</li><li>Structure the data model to answer the business questions</li><li>Identify data sources that have data that is needed</li><li>Transform and Load the data into the data model</li><li>Use the data model for analysis.</li></ul><div>In short,&nbsp;</div></div><div>Structure -&gt; Load -&gt; Analyze</div><div><br /></div><div>The new approach of data lake is originating from an approach wherein, it is attempted to reverse this. i.e.,</div><div>&nbsp;Load -&gt; Analyze -&gt; Structure</div><div><br /></div><div>This can seem a little confusing to begin with, but once you understand this difference, Data Lake will begin to make sense.&nbsp;</div><div><br /></div><div>You now:&nbsp;</div><div><ul style="text-align: left;"><li>Identify the conceptual entities needed and the systems that have it</li><li>Bring the data from these systems in their native format to data lake</li><li>Perform data mashups and analyze data to make sense out of what you need</li><li>Present analyses to the end users with real data</li><li>Materialize the data needed and the rest is already known :)</li></ul><div><br /></div></div></div>